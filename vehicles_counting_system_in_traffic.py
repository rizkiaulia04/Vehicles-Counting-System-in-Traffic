# -*- coding: utf-8 -*-
"""Vehicles Counting System in Traffic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L8-bCcU4sNQhz5eoZVfWAXr985BNasJY

### PROJECT AKHIR 
### COMPUTER VISION "VEHICLES COUNTING SYSTEM IN TRAFFIC"




---
# NAMA ANGGOTA KELOMPOK


1. Yuliano Komanjali
2. Rizki Aulia
3. Hafizh Julio Zalri
4. M. Hanaf Wal Ikhram
5. Rivani Widya Astuti
"""

# instal library dan unduh video untuk diproses
!pip install sk-video>=1.1.8 #menyediakan akses mudah ke rutinitas pemrosesan video
import os #mengimport modul OS(Operating System)
if not os.path.exists('road.mp4'):
   !wget https://learnml.s3.eu-north-1.amazonaws.com/road.mp4 #mengunduh video dari link web

"""# **Membuat Aplikasi Penghitung Kendaraan di Jalan Berbasis Computer Vision dan OpenCV**

Menghitung Kendaraan di Jalan menggunakan computer vision dan tanpa algoritma pembelajaran yang ribet

Hanya menggunakan Python dan OpenCV dengan ide deteksi gerakan yang cukup sederhana dengan bantuan algoritma Background Subtraction

Langkah pengerjaan :

1. Memahami ide utama algoritma Background Subtraction yang digunakan untuk deteksi foreground.
2. Filter gambar OpenCV.
3. Deteksi objek dengan kontur.
4. Membangun pipa pemrosesan untuk manipulasi data lebih lanjut.
"""

# import modul yang dibutuhkan

import os #import operating system
import csv #import text format csv
import numpy as np #proses data numerik
import logging 
import logging.handlers
import math #fungsi matematika
import sys #library untuk akses konfigurasi saat runtime
import random #menghasilkan angka acak yang bisa digunakan untuk berbagai keperluan
import numpy as np
import skvideo.io #untuk pemrosesan video
import cv2 #openCV
import matplotlib.pyplot as plt #pemrosesan plot pada gambar

from IPython.display import HTML #tampilan html pada python
from base64 import b64encode #encoding string pada python

# agar tidak terjadi beberapa kesalahan aneh
cv2.ocl.setUseOpenCL(False)
random.seed(123)

# setup logging
def init_logging(level=logging.INFO):
    main_logger = logging.getLogger()
    for hnd in main_logger.handlers:
        main_logger.removeHandler(hnd)

    formatter = logging.Formatter(
        fmt='%(asctime)s.%(msecs)03d %(levelname)-8s [%(name)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

    handler_stream = logging.StreamHandler(sys.stdout)
    handler_stream.setFormatter(formatter)
    main_logger.addHandler(handler_stream)
    main_logger.setLevel(level)

    return main_logger

"""## algoritma subtraction background


<img src="https://miro.medium.com/max/473/0*iNYtQubKAtK0OGG5.png" />

Ada banyak algoritma yang berbeda untuk background subtraction, tetapi ide utamanya sangat sederhana.

Kita asumsikan kita memiliki video kamar, dan pada beberapa bingkai video ini tidak ada manusia & hewan peliharaan, jadi pada dasarnya statis, sebut saja background_layer. Jadi untuk mendapatkan objek yang bergerak pada video kita hanya perlu:

foreground_objects = current_frame - background_layer

Tetapi dalam beberapa kasus, kita tidak bisa mendapatkan bingkai statis karena pencahayaan dapat berubah, atau beberapa objek akan dipindahkan oleh seseorang, atau selalu ada gerakan, dll. Dalam kasus seperti itu, kami menyimpan sejumlah bingkai dan mencoba mencari tahu piksel mana yang sama untuk sebagian besar dari mereka, maka piksel ini menjadi bagian dari background_layer. Perbedaan umumnya dalam cara kita mendapatkan background_layer ini dan pemfilteran tambahan yang kita gunakan untuk membuat seleksi lebih akurat.

Kami menggunakan algoritma MOG untuk pengurangan latar belakang dan setelah diproses, terlihat seperti ini:
<img src="https://miro.medium.com/max/2560/1*UctRAiOt_tyXOJd4aX1rZg.png" />
"""

def train_bg_subtractor(inst, cap, num=500):
    '''
       BG Substractor perlu memproses sejumlah frame untuk mulai memberikan hasil
    '''
    print ('Training BG Subtractor...')
    i = 0
    for frame in cap:
        inst.apply(frame, None, 0.001)
        i += 1
        if i >= num:
            return cap

VIDEO_SOURCE = "road.mp4"

bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, detectShadows=True)

# Set up image source
cap = skvideo.io.vreader(VIDEO_SOURCE)

# skip 500 frames untuk train bg subtractor
train_bg_subtractor(bg_subtractor, cap, num=500)

frame = next(cap)
fg_mask = bg_subtractor.apply(frame, None, 0.001)

plt.figure(figsize=(12,12))
plt.imshow(fg_mask)
plt.show()

"""ada beberapa noise pada foreground mask yang akan kami coba hilangkan dengan beberapa teknik penyaringan standar.

## Filtering

kami membutuhkan filter ini: Threshold, Erode, Dilate, Opening, Closing. 

Jadi sekarang kami menggunakannya untuk menghilangkan beberapa noise pada foreground mask. Pertama, kita akan menggunakan Closing untuk menghilangkan celah di area, kemudian Open untuk menghilangkan titik 1-2 px, dan setelah itu dilation untuk membuat objek lebih jelas.
"""

def filter_mask(img):
    '''
        Filter ini digunakan berdasarkan tes visual
    '''

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))

    # Fill any small holes
    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
    # Remove noise
    opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)

    # Dilate to merge adjacent blobs
    dilation = cv2.dilate(opening, kernel, iterations=2)

    return dilation

bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, detectShadows=True)

# Set up image source
cap = skvideo.io.vreader(VIDEO_SOURCE)

# skipping 500 frames to train bg subtractor
train_bg_subtractor(bg_subtractor, cap, num=500)

frame = next(cap)
fg_mask = bg_subtractor.apply(frame, None, 0.001)
fg_mask[fg_mask < 240] = 0
fg_mask = filter_mask(fg_mask)

plt.figure(figsize=(12,12))
plt.imshow(fg_mask)
plt.show()

"""## Object detection dengan menggunakan contours

Untuk tujuan ini kita akan menggunakan metode cv2.findContours standar dengan params:

```
cv2.CV_RETR_EXTERNAL â€” get only outer contours.
cv2.CV_CHAIN_APPROX_TC89_L1 - use Teh-Chin chain approximation algorithm (faster)
```


"""

def get_centroid(x, y, w, h):
    x1 = int(w / 2)
    y1 = int(h / 2)

    cx = x + x1
    cy = y + y1

    return (cx, cy)


class ContourDetection:
    '''
        Detecting moving objects.

        Tujuan prosesor ini adalah untuk subtrac background, get moving objects
        dan mendeteksi mereka dengan sebuah metode cv2.findContours, lalu filter off-by
        width and height. 

        bg_subtractor - background subtractor isinstance.
        min_contour_width - min bounding rectangle width.
        min_contour_height - min bounding rectangle height.
        save_image - if True will save detected objects mask to file.
        image_dir - where to save images(must exist).        
    '''

    def __init__(self, bg_subtractor, min_contour_width=35, min_contour_height=35, save_image=False, image_dir='images'):
        super(ContourDetection, self).__init__()

        self.bg_subtractor = bg_subtractor
        self.min_contour_width = min_contour_width
        self.min_contour_height = min_contour_height
        self.save_image = save_image
        self.image_dir = image_dir

    def filter_mask(self, img, a=None):
        '''
            This filters are hand-picked just based on visual tests
        '''

        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))

        # Fill any small holes
        closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
        # Remove noise
        opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)

        # Dilate to merge adjacent blobs
        dilation = cv2.dilate(opening, kernel, iterations=2)

        return dilation

    def detect_vehicles(self, fg_mask):

        matches = []

        # finding external contours
        contours, hierarchy = cv2.findContours(
            fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1)

        for (i, contour) in enumerate(contours):
            (x, y, w, h) = cv2.boundingRect(contour)
            # On the exit, kami menambahkan beberapa pemfilteran berdasarkan tinggi, lebar, dan menambahkan centroid.
            contour_valid = (w >= self.min_contour_width) and (
                h >= self.min_contour_height)

            if not contour_valid:
                continue

            centroid = get_centroid(x, y, w, h)

            matches.append(((x, y, w, h), centroid))

        return matches

    def __call__(self, frame):
        frame = frame.copy()

        fg_mask = self.bg_subtractor.apply(frame, None, 0.001)
        # nilai ambang batas
        fg_mask[fg_mask < 240] = 0
        fg_mask = self.filter_mask(fg_mask, 0)

        return self.detect_vehicles(fg_mask)

cd = ContourDetection(bg_subtractor)

bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, detectShadows=True)

# Set up image source
cap = skvideo.io.vreader(VIDEO_SOURCE)

# skipping 500 frames to train bg subtractor
train_bg_subtractor(bg_subtractor, cap, num=500)

frame = next(cap)
objects = cd(frame)

print('Getting list of [((x,y,w,h), (xc,yc)), ...]')
print(objects)

"""## Building processing pipeline

Anda harus memahami bahwa dalam ML dan CV tidak ada satu algoritma ajaib yang membuat semuanya, bahkan jika kita membayangkan bahwa algoritma seperti itu ada, kita tetap tidak akan menggunakannya karena tidak akan efektif dalam skala. Contohnya beberapa tahun yang lalu Netflix membuat kompetisi dengan hadiah 3 juta dolar untuk algoritma rekomendasi film terbaik. Dan salah satu tim menciptakan masalah seperti itu, tidak dapat bekerja dalam skala besar dan dengan demikian tidak berguna bagi perusahaan. Tapi tetap saja, Netflix membayar 1 juta kepada mereka :)

Jadi sekarang kita akan membangun pipa pemrosesan sederhana, bukan untuk skala hanya untuk kenyamanan tetapi idenya sama.

"""

class PipelineRunner(object):
    '''
        Very simple pipline.

        Just run passed processors in order with passing context from one to 
        another.

        You can also set log level for processors.
    '''

    def __init__(self, pipeline=None, log_level=logging.INFO):
        self.pipeline = pipeline or []
        self.context = {}
        self.log = logging.getLogger(self.__class__.__name__)
        self.log.setLevel(log_level)
        self.log_level = log_level
        self.set_log_level()

    def set_context(self, data):
        self.context = data

    def add(self, processor):
        if not isinstance(processor, PipelineProcessor):
            raise Exception(
                'Processor should be an isinstance of PipelineProcessor.')
        processor.log.setLevel(self.log_level)
        self.pipeline.append(processor)

    def remove(self, name):
        for i, p in enumerate(self.pipeline):
            if p.__class__.__name__ == name:
                del self.pipeline[i]
                return True
        return False

    def set_log_level(self):
        for p in self.pipeline:
            p.log.setLevel(self.log_level)

    def run(self):
        for p in self.pipeline:
            self.context = p(self.context)

        self.log.debug("Frame #%d processed.", self.context['frame_number'])

        return self.context


class PipelineProcessor(object):
    '''
        Base class for processors.
    '''

    def __init__(self):
        self.log = logging.getLogger(self.__class__.__name__)

"""Sebagai input konstruktor akan mengambil daftar prosesor yang akan dijalankan secara berurutan. Setiap prosesor membuat bagian dari pekerjaan.

Kami sudah memiliki kelas Deteksi Hitungan, hanya perlu sedikit memperbaruinya untuk menggunakan konteks
"""

def save_frame(frame, file_name, flip=True):
    # flip BGR to RGB
    if flip:
        cv2.imwrite(file_name, np.flip(frame, 2))
    else:
        cv2.imwrite(file_name, frame)

class ContourDetection(PipelineProcessor):
    '''
        Detecting moving objects.

        Purpose of this processor is to subtrac background, get moving objects
        and detect them with a cv2.findContours method, and then filter off-by
        width and height. 

        bg_subtractor - background subtractor isinstance.
        min_contour_width - min bounding rectangle width.
        min_contour_height - min bounding rectangle height.
        save_image - if True will save detected objects mask to file.
        image_dir - where to save images(must exist).        
    '''

    def __init__(self, bg_subtractor, min_contour_width=35, min_contour_height=35, save_image=False, image_dir='images'):
        super(ContourDetection, self).__init__()

        self.bg_subtractor = bg_subtractor
        self.min_contour_width = min_contour_width
        self.min_contour_height = min_contour_height
        self.save_image = save_image
        self.image_dir = image_dir

    def filter_mask(self, img, a=None):
        '''
            This filters are hand-picked just based on visual tests
        '''

        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))

        # Fill any small holes
        closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
        # Remove noise
        opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)

        # Dilate to merge adjacent blobs
        dilation = cv2.dilate(opening, kernel, iterations=2)

        return dilation

    def detect_vehicles(self, fg_mask, context):

        matches = []

        # finding external contours
        contours, hierarchy = cv2.findContours(
            fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1)

        for (i, contour) in enumerate(contours):
            (x, y, w, h) = cv2.boundingRect(contour)
            contour_valid = (w >= self.min_contour_width) and (
                h >= self.min_contour_height)

            if not contour_valid:
                continue

            centroid = get_centroid(x, y, w, h)

            matches.append(((x, y, w, h), centroid))

        return matches

    def __call__(self, context):
        frame = context['frame'].copy()
        frame_number = context['frame_number']

        fg_mask = self.bg_subtractor.apply(frame, None, 0.001)
        # just thresholding values
        fg_mask[fg_mask < 240] = 0
        fg_mask = self.filter_mask(fg_mask, frame_number)

        if self.save_image:
            save_frame(fg_mask, self.image_dir +
                             "/mask_%04d.png" % frame_number, flip=False)

        context['objects'] = self.detect_vehicles(fg_mask, context)
        context['fg_mask'] = fg_mask

        return context

"""Sekarang mari kita buat prosesor yang akan menghubungkan objek yang terdeteksi pada bingkai yang berbeda dan akan membuat jalur, dan juga akan menghitung kendaraan yang sampai keluar zona.

"""

def distance(x, y, type='euclidian', x_weight=1.0, y_weight=1.0):
    if type == 'euclidian':
        return math.sqrt(float((x[0] - y[0])**2) / x_weight + float((x[1] - y[1])**2) / y_weight)


class VehicleCounter(PipelineProcessor):
    '''
        Menghitung kendaraan yang masuk di exit zone.

        Tujuan kelas ini berdasarkan objek yang terdeteksi dan pembuatan cache lokal
        jalur objek dan jumlah yang masuk di zona keluar yang ditentukan oleh exit masks.

        exit_masks - list of the exit masks.
        path_size - max number of points in a path.
        max_dst - max distance between two points.
    '''

    def __init__(self, exit_masks=[], path_size=10, max_dst=30, x_weight=1.0, y_weight=1.0):
        super(VehicleCounter, self).__init__()

        self.exit_masks = exit_masks

        self.vehicle_count = 0
        self.path_size = path_size
        self.pathes = []
        self.max_dst = max_dst
        self.x_weight = x_weight
        self.y_weight = y_weight

    def check_exit(self, point):
        for exit_mask in self.exit_masks:
            try:
                if exit_mask[point[1]][point[0]] == 255:
                    return True
            except:
                return True
        return False

    def __call__(self, context):
        objects = context['objects']
        context['exit_masks'] = self.exit_masks
        context['pathes'] = self.pathes
        context['vehicle_count'] = self.vehicle_count
        if not objects:
            return context

        points = np.array(objects)[:, 0:2]
        points = points.tolist()

        # tambahkan poin baru jika jalurnya kosong
        if not self.pathes:
            for match in points:
                self.pathes.append([match])

        else:
            # hubungkan titik baru dengan jalur lama berdasarkan jarak minimum antar titik
            new_pathes = []

            for path in self.pathes:
                _min = 999999
                _match = None
                for p in points:
                    if len(path) == 1:
                        # jarak dari titik terakhir ke saat ini
                        d = distance(p[0], path[-1][0])
                    else:
                        # berdasarkan 2 poin sebelumnya yang akan memprediksi titik berikutnya dan menghitung jarak dari prediksi titik berikutnya ke saat ini
                        xn = 2 * path[-1][0][0] - path[-2][0][0]
                        yn = 2 * path[-1][0][1] - path[-2][0][1]
                        d = distance(
                            p[0], (xn, yn),
                            x_weight=self.x_weight,
                            y_weight=self.y_weight
                        )

                    if d < _min:
                        _min = d
                        _match = p

                if _match and _min <= self.max_dst:
                    points.remove(_match)
                    path.append(_match)
                    new_pathes.append(path)

                # jika frame tidak cocok, maka path tidak di drop lalu tambahkan path baru
                if _match is None:
                    new_pathes.append(path)

            self.pathes = new_pathes

            # add new pathes
            if len(points):
                for p in points:
                    # jangan menambah poin yang sudah seharusnya dihitung
                    if self.check_exit(p[1]):
                        continue
                    self.pathes.append([p])

        # Hanya menyimpan point N terakhir di path
        for i, _ in enumerate(self.pathes):
            self.pathes[i] = self.pathes[i][self.path_size * -1:]

        # count vehicles and drop counted pathes:
        new_pathes = []
        for i, path in enumerate(self.pathes):
            d = path[-2:]

            if (
                # perlu 2 list untuk dihitung
                len(d) >= 2 and
                # titik sebelumnya tidak di exit zone
                not self.check_exit(d[0][1]) and
                # titik sekarang di exit zone
                self.check_exit(d[1][1]) and
                # path len lebih besar dari min
                self.path_size <= len(path)
            ):
                self.vehicle_count += 1
            else:
                # mencegah menghubungkan dengan jalur yang sudah di exit zone
                add = True
                for p in path:
                    if self.check_exit(p[1]):
                        add = False
                        break
                if add:
                    new_pathes.append(path)

        self.pathes = new_pathes

        context['pathes'] = self.pathes
        context['objects'] = objects
        context['vehicle_count'] = self.vehicle_count

        self.log.debug('#VEHICLES FOUND: %s' % self.vehicle_count)

        return context

"""Kelas ini agak rumit jadi kita telusuri bagian demi bagian.

<img src="https://miro.medium.com/max/1280/1*yTW0iqr8QK24Fkb6JJHhLw.png" />

greenmask pada gambar ini adalah zona keluar, tempat kami menghitung kendaraan. Misalnya, kami hanya akan menghitung kendaraan yang memiliki panjang lebih dari 3 titik (untuk menghilangkan beberapa noise) dan yang ke-4 di zona hijau.
Kami menggunakan mask karena banyak operasi yang efektif dan lebih sederhana daripada menggunakan algoritma vektor.

Cukup gunakan operasi "biner and" untuk memeriksa titik itu di area tersebut, dan itu saja. Dan inilah cara kami mengaturnya:
"""

EXIT_PTS = np.array([
    [[732, 720], [732, 590], [1280, 500], [1280, 720]],
    [[0, 400], [645, 400], [645, 0], [0, 0]]
])
SHAPE = (720,1280)
base = np.zeros(SHAPE + (3,), dtype='uint8')
exit_mask = cv2.fillPoly(base, EXIT_PTS, (255, 255, 255))[:, :, 0]

plt.imshow(base)
plt.show()

"""Sekarang kita hubungkan titik-titik di jalur pada baris 55

Pada bingkai pertama. kami hanya menambahkan semua poin sebagai jalur baru.

Selanjutnya if len(path) == 1, untuk setiap path dalam cache kita mencoba mencari titik(centroid) dari objek yang baru terdeteksi yang akan memiliki jarak Euclidean terkecil ke titik terakhir dari path.

Jika len(jalur) > 1, maka dengan dua titik terakhir di lintasan kita memprediksi titik baru pada garis yang sama, dan menemukan jarak minimum antara titik itu dan titik saat ini.

Titik dengan jarak minimal ditambahkan ke ujung jalur saat ini dan dihapus dari daftar.

Jika beberapa poin tersisa setelah ini, kami menambahkannya sebagai jalur baru.

Dan juga kami membatasi jumlah titik di jalur pada baris 101

Sekarang kita coba hitung kendaraan yang masuk di exit zone. Untuk melakukan ini, kita hanya mengambil 2 titik terakhir di jalur dan memeriksa yang terakhir di zona keluar, dan sebelumnya tidak, dan juga memeriksa bahwa len(jalur) harus lebih besar dari batas. Bagian demi bagian adalah mencegah menghubungkan kembali titik-titik baru ke titik-titik di zona keluar.

Dan dua prosesor terakhir adalah penulis CSV untuk membuat file CSV laporan, dan visualisasi untuk debugging dan gambar/video yang bagus.
"""

class CsvWriter(PipelineProcessor):

    def __init__(self, path, name, start_time=0, fps=15):
        super(CsvWriter, self).__init__()

        self.fp = open(os.path.join(path, name), 'w')
        self.writer = csv.DictWriter(self.fp, fieldnames=['time', 'vehicles'])
        self.writer.writeheader()
        self.start_time = start_time
        self.fps = fps
        self.path = path
        self.name = name
        self.prev = None

    def __call__(self, context):
        frame_number = context['frame_number']
        count = _count = context['vehicle_count']

        if self.prev:
            _count = count - self.prev

        time = ((self.start_time + int(frame_number / self.fps)) * 100 
                + int(100.0 / self.fps) * (frame_number % self.fps))
        self.writer.writerow({'time': time, 'vehicles': _count})
        self.prev = count

        return context

BOUNDING_BOX_COLOUR = (255, 192, 0)
CENTROID_COLOUR = (255, 192, 0)
CAR_COLOURS = [(255, 192, 0)]
EXIT_COLOR = (66, 183, 42)

class Visualizer(PipelineProcessor):

    def __init__(self, save_image=True, image_dir='images'):
        super(Visualizer, self).__init__()

        self.save_image = save_image
        self.image_dir = image_dir

    def check_exit(self, point, exit_masks=[]):
        for exit_mask in exit_masks:
            if exit_mask[point[1]][point[0]] == 255:
                return True
        return False

    def draw_pathes(self, img, pathes):
        if not img.any():
            return

        for i, path in enumerate(pathes):
            path = np.array(path)[:, 1].tolist()
            for point in path:
                cv2.circle(img, point, 2, CAR_COLOURS[0], -1)
                cv2.polylines(img, [np.int32(path)], False, CAR_COLOURS[0], 1)

        return img

    def draw_boxes(self, img, pathes, exit_masks=[]):
        for (i, match) in enumerate(pathes):

            contour, centroid = match[-1][:2]
            if self.check_exit(centroid, exit_masks):
                continue

            x, y, w, h = contour

            cv2.rectangle(img, (x, y), (x + w - 1, y + h - 1),
                          BOUNDING_BOX_COLOUR, 1)
            cv2.circle(img, centroid, 2, CENTROID_COLOUR, -1)

        return img

    def draw_ui(self, img, vehicle_count, exit_masks=[]):

        # this just add green mask with opacity to the image
        for exit_mask in exit_masks:
            _img = np.zeros(img.shape, img.dtype)
            _img[:, :] = EXIT_COLOR
            mask = cv2.bitwise_and(_img, _img, mask=exit_mask)
            cv2.addWeighted(mask, 1, img, 1, 0, img)

        # drawing top block with counts
        cv2.rectangle(img, (0, 0), (img.shape[1], 50), (0, 0, 0), cv2.FILLED)
        cv2.putText(img, ("Vehicles passed: {total} ".format(total=vehicle_count)), (30, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
        return img

    def __call__(self, context):
        frame = context['frame'].copy()
        frame = np.ascontiguousarray(np.flip(frame, 2))
        frame_number = context['frame_number']
        pathes = context['pathes']
        exit_masks = context['exit_masks']
        vehicle_count = context['vehicle_count']

        frame = self.draw_ui(frame, vehicle_count, exit_masks)
        frame = self.draw_pathes(frame, pathes)
        frame = self.draw_boxes(frame, pathes, exit_masks)
        if self.save_image:
            save_frame(frame, self.image_dir +
                            "/processed_%04d.png" % frame_number)

        context['frame'] = frame
        return context

"""Penulis CSV menyimpan data berdasarkan waktu, karena kami membutuhkannya untuk analisis lebih lanjut. Jadi saya menggunakan rumus ini untuk menambahkan waktu bingkai tambahan ke unixtimestamp:

```
time = ((self.start_time + int(frame_number / self.fps)) * 100 
                + int(100.0 / self.fps) * (frame_number % self.fps))
```

```
so with start time=1 000 000 000 and fps=10 i will get results like this
frame 1 = 1 000 000 000 010
frame 1 = 1 000 000 000 020
â€¦
```



Kemudian setelah Anda mendapatkan laporan csv lengkap, Anda dapat menggabungkan data ini sesuai keinginan.

## Conclusion
Jadi seperti yang Anda lihat, itu tidak sesulit yang dipikirkan banyak orang.

Tetapi jika Anda menjalankan skrip, Anda akan melihat bahwa solusi ini tidak ideal, dan memiliki masalah dengan objek latar depan yang tumpang tindih, juga tidak memiliki klasifikasi kendaraan berdasarkan jenis (yang pasti Anda perlukan untuk analitik nyata). Tapi tetap saja, dengan posisi kamera yang bagus (di atas jalan), ini memberikan akurasi yang cukup baik. Dan itu memberitahu kita bahwa bahkan algoritma kecil & sederhana yang digunakan dengan cara yang benar dapat memberikan hasil yang baik.

**Jadi apa yang bisa kita lakukan untuk memperbaiki masalah saat ini?**

Salah satu caranya adalah dengan mencoba menambahkan beberapa filtrasi tambahan yang mencoba memisahkan objek untuk deteksi yang lebih baik. Cara lainnya adalah dengan menggunakan algoritme yang lebih kompleks seperti jaringan konvolusi dalam.
"""

# build runner
def main():
    log = logging.getLogger("main")

    # creating exit mask from points, where we will be counting our vehicles
    base = np.zeros(SHAPE + (3,), dtype='uint8')
    exit_mask = cv2.fillPoly(base, EXIT_PTS, (255, 255, 255))[:, :, 0]

    # there is also bgslibrary, that seems to give better BG substruction, but
    # not tested it yet
    bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, detectShadows=True)

    # processing pipline for programming conviniance
    pipeline = PipelineRunner(pipeline=[
        ContourDetection(bg_subtractor=bg_subtractor,
                         save_image=True, image_dir=IMAGE_DIR),
        # we use y_weight == 2.0 because traffic are moving vertically on video
        # use x_weight == 2.0 for horizontal.
        VehicleCounter(exit_masks=[exit_mask], y_weight=2.0),
        Visualizer(image_dir=IMAGE_DIR,save_image=False),
        CsvWriter(path='./', name='report.csv')
    ], log_level=logging.INFO)

    # Set up image source
    cap = skvideo.io.vreader(VIDEO_SOURCE)

    # skipping 500 frames to train bg subtractor
    train_bg_subtractor(bg_subtractor, cap, num=500)

    fourcc = cv2.VideoWriter_fourcc(*"MP4V")
    writer = cv2.VideoWriter(VIDEO_OUT, fourcc, 25, (SHAPE[1], SHAPE[0]), True)

    frame_number = -1
    for frame in cap:
        if not frame.any():
            log.error("Frame capture failed, stopping...")
            break

        frame_number += 1
        log.info("Frame #%s" % frame_number)

        pipeline.set_context({
            'frame': frame,
            'frame_number': frame_number,
        })
        ctx = pipeline.run()
        writer.write(ctx['frame'])

        if frame_number > PARSE_FRAMES:
            break
    writer.release()

# Parameters
# ============================================================================
IMAGE_DIR = "./out"
VIDEO_SOURCE = "road.mp4"
VIDEO_OUT = "road_parsed.mp4"
PARSE_FRAMES = 15*25
SHAPE = (720, 1280)  # HxW
EXIT_PTS = np.array([
    [[732, 720], [732, 590], [1280, 500], [1280, 720]],
    [[0, 400], [645, 400], [645, 0], [0, 0]]
])
# ============================================================================

log = init_logging()
main()

from google.colab import files
files.download('road_parsed.mp4')